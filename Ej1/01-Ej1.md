# üß™ Ejercicio 1 ‚Äì Replicaci√≥n de Base de Datos (PostgreSQL ‚Üí Supabase)

Este m√≥dulo resuelve el primer ejercicio de la prueba t√©cnica: replicar datos desde una base transaccional en PostgreSQL hacia una base espejo en la nube (Supabase), con automatizaci√≥n diaria.

---

## üß† ¬øQu√© ped√≠a el ejercicio?

La empresa ya cuenta con una base de datos transaccional que registra ventas y productos. Lo que necesitan es una **copia diaria de esa informaci√≥n** en la nube, para usarla con fines de an√°lisis.

> Mi enfoque fue crear una carga inicial y luego una carga incremental eficiente. Evitar hacer un full refresh todos los d√≠as es clave para no sobrecargar el sistema.

---

## üß± Paso 1: Crear el contenedor de PostgreSQL

Se levant√≥ un contenedor de PostgreSQL en Docker y se configur√≥ con una **red interna (`bridge`)**. Esto significa que **solo los servicios corriendo dentro del entorno de Docker pueden acceder al Data Warehouse**.

> Se uso para limitar el acceso, y es una buena pr√°ctica de encapsulamiento de red para que se comuniquen mis servicios.

---

## üì• Paso 2: Leer los datos desde la base origen

Us√© `pandas` para leer los CSV y transformarlos en `DataFrames`. Luego, con `sqlalchemy` + `pymysql` cre√© una conexi√≥n entre el contenedor de Python y el contenedor de PostgreSQL.

> Como ambos contenedores est√°n en la misma red, la conexi√≥n fue directa y sin exponer puertos p√∫blicos.

```python
df = pd.read_csv(f)# f es el .csv
df.columns = [col.lower() for col in df.columns]#Para mas control convertimos las columnas a minusculas 
table_name = os.path.splitext(os.path.basename(f))[0].lower()#toma el nombre del archivo como nombre de la tabla
df.to_sql(table_name, engine, if_exists='replace', index=False)
```

Los archivos CSV utilizados fueron mapeados y cargados en el volumen para poder usarlos

üì∏ Al ejecutar el dag sigue el siguiente flujo con exito:

> Si `la base de datos creada` esta vacia, el script asume que es la **primera carga**. Si ya tiene datos, realiza una carga **incremental**, ahorrando recursos.


![StgLog](../src/CargaInicialflujo.png)

Resultado: 

![Docker Bridge](../src/CargaDW.png)

---

## ‚òÅÔ∏è Paso 3: Replicar la base en la nube (Supabase)

Antes se debe crear un proyecto en Supabase, con las tablas cargadas previamente ![Codigo para crear las tablas](.SupabaseCopia.sql)respetando el modelo estrella. Este fue el modelo l√≥gico interpretado:

```
DimProduct(productid PK, producttype)
DimCustomer(segmentid PK, city)
DimDate(dateid PK, quarter, date, monthname, month, year, quartername)
FactSales(saleid PK, segmentid FK, productid FK, dateid FK, price_per_unit, quantity_sold)
```

üì∏ MER:

![Modelo ENTIDAD RELACION](../src/MER.png)

---

## üõ†Ô∏è Tablas auxiliares:
Realizaremos Cargas Incrementales para una mejor eficiencia del codigo ya que se realizan copias todos los dias

### `stg_log`
Esta tabla guarda registros de cu√°ndo se hizo la √∫ltima carga, qu√© tabla se actualiz√≥ y cu√°l fue el √∫ltimo ID.

---

### `stg_customersegment` y `stg_product`

Estas dos tablas permiten detectar cambios en los datos de dimensiones.Comparo los registros nuevos y guardo un `estado` (activo/inactivo).

> Esto es util si borraron datos en origen. Asi evito romper la tabla `factsales`, que necesita esas claves foraneas.

üì∏

![StgCustomer](../src/TableStgCustomer.png) ![StgDimProduct](../src/stg_dimproduct.png) 
---

## üîÑ Flujo general del script
### Carga inicial
![Carga inicial](../src/PrimerCarga.png)

### Tablas DimDate y FactSales
### Carga incremental 
![Carga incremental DimDate y FactSales](../src/PrimerCargaIncremental.png)



### Tablas DimCustomers y DimProductos
### Carga Incremental
![Carga inicial y Incremental de DimDate y FactSales](../src/CargaInicialyIncremental.png)


### Explicacion del script de comparacion:
Se realiza un **full outer join**, lo que permite identificar que datos de la primera tabla no estan en la segunda y viceversa.


![Full-outer join ej con DimCustomers](../src/Comparacion.png) 
---


## üìÖ Automatizaci√≥n con Airflow

El DAG corre todos los d√≠as y sigue el flujo dectectando la carga incial(solo una vez) o la carga incremental(n)

> Aunque podr√≠a no ser estrictamente necesario la columna fecha de la tabla `stg_log` me sirve para monitorear que todo corri√≥ bien asegurando que airflow corrio cada dia.

